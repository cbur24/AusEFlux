{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Historical Predictions of AusEFlux <img align=\"right\" src=\"https://github.com/cbur24/AusEFlux/blob/master/results/banner_picture.png?raw=True\" width=\"40%\">\n",
    "\n",
    "This notebook contains the workflow for creating historical carbon and water fluxes for Australia through the full length of the MODIS archive (i.e., 2003-2022). It contains six main steps, instructions are provided in the subsections below. Pay close attention to the `Analysis Parameters` sections and ensure paths etc. are correct.\n",
    "\n",
    "***\n",
    "**Ideal compute environment:**\n",
    "\n",
    "Assuming 5-km resolution\n",
    "\n",
    "- NCI's 'normal' queue\n",
    "- X-large (24 cores, 95GiB)\n",
    "- Python 3.10.0\n",
    "- Python venv: `/g/data/os22/chad_tmp/AusEFlux/env/py310`\n",
    "- Folders: `gdata/os22+gdata/ub8+gdata/xc0+gdata/gh70`\n",
    "***\n",
    "<!-- > **Expected completion time to run all steps: ~3 hours** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries and set up Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up project directory structure\n",
    "\n",
    "This workflow assumes a specific file/folder structure, here we create that folder structure to support the rest of the process.\n",
    "\n",
    "Below, enter the `root directory location` where project results and data are stored. If the folders alredy exist then no directories will be created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base='/g/data/os22/chad_tmp/AusEFlux/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import create_project_directories\n",
    "create_project_directories(root_dir=base)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Spatiotemporal harmonisation of input datasets\n",
    "\n",
    "Most datasets are originally from here: https://dapds00.nci.org.au/thredds/catalog/ub8/au/catalog.html\n",
    "\n",
    "Dataset from this process are output as annual layers in `data/interim`\n",
    "\n",
    "<!-- **Expected completion time ~2hrs** -->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `base`: Path to where most of the data is stored\n",
    "* `results`: Path to store interim datasets after they have undergone harmonisation.\n",
    "* `year_start`: The first year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/ub8/au/'\n",
    "results='/g/data/os22/chad_tmp/AusEFlux/data/interim/'\n",
    "year_start = 2003\n",
    "year_end = 2022"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _harmonisation import spatiotemporal_harmonisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spatiotemporal_harmonisation(\n",
    "    year_start=year_start,\n",
    "    year_end=year_end,\n",
    "    base_path=base,\n",
    "    results_path=results,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create feature datasets\n",
    "\n",
    "Combine results of the spatiotemporal harmonisation into temporally stacked netcdf files, and create new features/variables based on the climate (e.g. anomalies) and remote sensing (e.g veg fractions) datasets. \n",
    "\n",
    "**Expected completion time ~6 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `base`: Path to where the harmonised datasets output from Step 1 are stored. \n",
    "* `results`: Path to store temporally stacked netcdf files i.e. where the outputs of Step 2 will be stored\n",
    "* `exclude`: Variables to exclude from combining. i.e. Some of the variables in `/interim` output in Step 1 are not needed hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/AusEFlux/data/interim/'\n",
    "results='/g/data/os22/chad_tmp/AusEFlux/data/5km/'\n",
    "exclude = ['.ipynb_checkpoints', 'kTavg', 'Tmax', 'Tmin', 'EVI']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _feature_datasets import create_feature_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_datasets(\n",
    "    base=base,\n",
    "    results_path=results,\n",
    "    exclude=exclude,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Extract training data\n",
    "\n",
    "Scrape the TERN server to extract all of the ozflux eddy covariance data, then append remote sensing data by using the coordinates of the flux tower to extract pixel values. \n",
    "\n",
    "**Expected completion time ~12 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `version`: Version of OzFlux datasets to use, always has the form 'YYYY_v[number]'\n",
    "* `level`: What level of OzFlux data to use, level 6 is the highest level and has been pre-processed to 'analysis ready'\n",
    "* `type` : Ozflux data comes as either 'default' or 'site_pi' depending on how it was processed.\n",
    "* `rs_data_folder`: Where are the spatiotemporally harmonised and stacked feature layers that we will append to the EC data? The code simple loops through all netcdf files and appends the data. We can filter for features later on.\n",
    "* `save_ec_data`: If this variables is not 'None', then the EC netcdf files will be exported to this folder.\n",
    "* `export_path`: Where should we save the .csv files that contain the EC and RS data? i.e. this is our training data\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "version='2023_v1'\n",
    "level='L6'\n",
    "type='default'\n",
    "rs_data_folder='/g/data/os22/chad_tmp/AusEFlux/data/5km/'\n",
    "save_ec_data='/g/data/os22/chad_tmp/AusEFlux/data/ozflux_netcdf/'\n",
    "export_path='/g/data/os22/chad_tmp/AusEFlux/data/training_data/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _training import extract_ozflux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "extract_ozflux(\n",
    "    version=version,\n",
    "    level=level,\n",
    "    type=type,\n",
    "    rs_data_folder=rs_data_folder,\n",
    "    save_ec_data=save_ec_data,\n",
    "    export_path=export_path,\n",
    "    return_coords=True,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a plot of all the OzFlux sites (Optional)\n",
    "\n",
    "This is helpful in ensuring the site locations are in the correct places - sometimes OzFlux coordinates are incorrect.\n",
    "\n",
    "We also export a .csv with the site locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "site_export = '/g/data/os22/chad_tmp/AusEFlux/data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import contextily as ctx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sites = os.listdir(export_path)\n",
    "\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        xx = pd.read_csv(export_path+site)\n",
    "        xx['site'] = site[0:-4]\n",
    "        xx = xx[['site', 'x_coord', 'y_coord']]\n",
    "        xx=xx.head(1)\n",
    "        td.append(xx)\n",
    "\n",
    "df = pd.concat(td).dropna()\n",
    "print('n_sites:', len(df))\n",
    "\n",
    "#export site list to file\n",
    "df.to_csv(site_export+'ozflux_site_locations.csv')\n",
    "\n",
    "gdf = gpd.GeoDataFrame(\n",
    "    df, geometry=gpd.points_from_xy(df.x_coord, df.y_coord), crs=\"EPSG:4326\"\n",
    ")\n",
    "\n",
    "ax = gdf.plot(column='site', figsize=(10,10))\n",
    "gdf.apply(lambda x: ax.annotate(text=x['site'],\n",
    "            xy=x.geometry.centroid.coords[0],\n",
    "            ha='right', fontsize=8), axis=1);\n",
    "\n",
    "# Adding basemap might fail with max retries...something is wrong with contextily backend\n",
    "ctx.add_basemap(ax, source=ctx.providers.Esri.WorldImagery, crs='EPSG:4326', attribution='', attribution_size=1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Generate ensemble of Models\n",
    "\n",
    "We will attempt to model a portion of the empirical uncertainty that comes from the training data. To do this, we will generate 15 models. For each iteration, two flux tower sites  will be removed from the training data and an LGBM and RF model will be fit on the remaining data.  This will result in 30 models that later we can use to make 30 predictions. The IQR envelope of our predictions will inform our uncertainity\n",
    "\n",
    "> Note, before running this section shutdown any dask cluster that is running using `client.shutdown()`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# client.shutdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'ER' #ER NEE ET GPP\n",
    "n_iter = 100 #how many hyperparameter iterations to test for the final model fitting?\n",
    "n_models = 15 #how many iterations of models to create (iterations of training data)?\n",
    "n_cpus = 24\n",
    "\n",
    "base = '/g/data/os22/chad_tmp/AusEFlux/'\n",
    "\n",
    "ec_exclusions=['DalyUncleared', 'RedDirtMelonFarm', 'Loxton']\n",
    "\n",
    "modelling_vars = ['LST_RS', 'Î”T_RS',\n",
    "                  'kNDVI_RS','kNDVI_anom_RS',\n",
    "                  'NDWI_RS','NDWI_anom_RS',\n",
    "                  'trees_RS', 'grass_RS', 'bare_RS', 'C4_grass_RS',\n",
    "                  'rain_RS', 'rain_cml3_RS', 'rain_anom_RS',\n",
    "                  'rain_cml3_anom_RS', 'rain_cml6_anom_RS', 'rain_cml12_anom_RS',\n",
    "                  'SRAD_RS', 'SRAD_anom_RS',\n",
    "                  'Tavg_RS', 'Tavg_anom_RS',\n",
    "                  'VPD_RS', 'VPD_anom_RS',\n",
    "                  'VegH_RS', 'site'\n",
    "                ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comibine EC site data into a big pandas df------------------------\n",
    "sites = os.listdir(f'{base}data/training_data/')\n",
    "fluxes=['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC','ET_EC']\n",
    "td = []\n",
    "for site in sites:\n",
    "    if '.csv' in site:\n",
    "        if any(exc in site for exc in ec_exclusions): #don't load the excluded sites\n",
    "            print('skip', site[0:-4])\n",
    "            continue\n",
    "        else:\n",
    "            xx = pd.read_csv(f'{base}data/training_data/{site}',\n",
    "                             index_col='time', parse_dates=True)\n",
    "            xx['site'] = site[0:-4]\n",
    "            xx = xx[fluxes+modelling_vars]\n",
    "            td.append(xx)\n",
    "\n",
    "ts = pd.concat(td).dropna() #we'll use this later\n",
    "\n",
    "# convert pandas df into sklearn X, y --------------------------\n",
    "xx = []\n",
    "yy = []\n",
    "for t in td:    \n",
    "    t = t.dropna()  # remove NaNS\n",
    "    df = t.drop(['NEE_SOLO_EC','GPP_SOLO_EC','ER_SOLO_EC'],\n",
    "                axis=1) # seperate carbon fluxes\n",
    "    \n",
    "    df = df[modelling_vars]\n",
    "    \n",
    "    if model_var == 'ET':\n",
    "        df_var=t[[model_var+'_EC', 'site']]\n",
    "    else:\n",
    "        df_var=t[[model_var+'_SOLO_EC', 'site']]\n",
    "    \n",
    "    x = df.reset_index(drop=True)\n",
    "    y = df_var.reset_index(drop=True)\n",
    "    xx.append(x)\n",
    "    yy.append(y)\n",
    "\n",
    "x = pd.concat(xx)\n",
    "y = pd.concat(yy)\n",
    "print(x.shape)\n",
    "\n",
    "#export features list ----------------------------------\n",
    "textfile = open(f'{base}results/variables.txt', 'w')\n",
    "for element in x.columns:\n",
    "    textfile.write(element + \",\")\n",
    "textfile.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 4\n",
    "\n",
    "Note, it will take many hours to create 30 unique models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _ensemble_modelling import ensemble_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_models(\n",
    "    base=base,\n",
    "    model_var=model_var,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    n_cpus=n_cpus,\n",
    "    n_iter=n_iter,\n",
    "    n_models=n_models,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create validaton plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _ensemble_modelling import validation_plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_plots(\n",
    "    base=base,\n",
    "    model_var=model_var\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create ensemble feature importance plots\n",
    "\n",
    "> Note, the RF models are very slow to process, so this can take several hours to complete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _ensemble_modelling import ensemble_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_feature_importance(\n",
    "    base=base,\n",
    "    model_var=model_var,\n",
    "    x=x,\n",
    "    y=y,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Predict ensemble\n",
    "\n",
    "Using the ensemble of models, we will generate an ensemble of gridded predictions.\n",
    "\n",
    "**Expected completion time ~45 mins/year**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `model_var`: Which variable are we modelling? Must be one of 'GPP', 'ER', 'NEE', or 'ET'\n",
    "* `base`: Path to where the harmonised datasets output from Step 1 are stored. \n",
    "* `results_path`: Path to store temporally stacked netcdf files i.e. where the outputs of Step 2 will be stored\n",
    "* `year_start`: The first year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `models_folder`: where are the models stored?\n",
    "* `features_list`: Where are the list of features used by the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'ER' #ER #NEE #ET #GPP\n",
    "base = '/g/data/os22/chad_tmp/AusEFlux/'\n",
    "year_start, year_end=2003, 2022\n",
    "results_path = f'{base}results/predictions/ensemble/historical/{model_var}/'\n",
    "models_folder = f'{base}results/models/ensemble/{model_var}/'\n",
    "features_list = f'{base}results/variables.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _ensemble_prediction import predict_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "predict_ensemble(\n",
    "   base=base,\n",
    "   model_var=model_var,\n",
    "   models_folder=models_folder,\n",
    "   features_list=features_list,\n",
    "   results_path=results_path,\n",
    "   year_start=year_start,\n",
    "   year_end=year_end,\n",
    "   compute_early=True,\n",
    "   verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Combine ensembles\n",
    "\n",
    "Ran an ensemble of predictions, now we need to compute the ensemble median and the uncertainty range.\n",
    "\n",
    "This step will also output production ready datasets with appropriate metadata\n",
    "\n",
    "**Expected completion time, 5 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `model_var`: Which variable are we combining? Must be one of 'GPP', 'ER', 'NEE', or 'ET'\n",
    "* `base`: Path to where the modelling/data etc is occuring. We build the other path strings from the 'base' path to reduce the length of path strings.\n",
    "* `results_path`: Path where final AusEFlux datasets will be output.\n",
    "* `year_start`: The first year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `quantiles`: What quantiles are we using to determine the middle value and uncertainty range? The default is 0.05 and 0.95 for the uncertainty envelope, and 0.5 (median) for the middle estimate. You're advised not to change these.\n",
    "* `predictions_folder`: where are the ensemble predictions stored? Those output from the previous step.\n",
    "\n",
    "> There are also several metadata fields (e.g. `full_name`, `units`) that will change with the variable being modelled. Make sure you update these for each model run as these atttributes are appended to the exported netcdf files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/AusEFlux/'\n",
    "model_var = 'ER' #ER #NEE #ET #GPP\n",
    "results_path = f'{base}results/AusEFlux/{model_var}/'\n",
    "year_start, year_end=2003,2022\n",
    "quantiles=[0.25,0.5,0.75] # interquartile range\n",
    "# predictions_folder= f'{base}results/predictions/ensemble/annual_update/{year_start}/{model_var}/'\n",
    "predictions_folder= f'{base}results/predictions/ensemble/historical/{model_var}/'\n",
    "\n",
    "# metadata for netcdf attributes\n",
    "full_name = 'Ecosystem Respiration'#'Gross Primary Productivity' #Net Ecosystem Exchange #Ecosystem Respiration #Evapotranspiration\n",
    "version = 'v1.2'\n",
    "crs='EPSG:4326'\n",
    "units = 'gC/m\\N{SUPERSCRIPT TWO}/month' #mm/month\n",
    "description = f'AusEFlux {full_name} is created by empirically upscaling the OzFlux eddy covariance network using machine learning methods coupled with climate and remote sensing datasets. The estimates provided within this dataset were extracted from an ensemble of predictions and represent the median and uncertainty range.'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create attributes dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "attrs_dict={}\n",
    "attrs_dict['nodata'] = np.nan\n",
    "attrs_dict['crs'] = crs\n",
    "attrs_dict['short_name'] = model_var\n",
    "attrs_dict['long_name'] = full_name\n",
    "attrs_dict['units'] = units\n",
    "attrs_dict['version'] = version\n",
    "attrs_dict['description'] = description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _combine_ensemble import combine_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combine_ensemble(\n",
    "    base=base,\n",
    "    model_var=model_var,\n",
    "    results_path=results_path,\n",
    "    predictions_folder=predictions_folder,\n",
    "    year_start=year_start,\n",
    "    year_end=year_end,\n",
    "    attrs=attrs_dict,\n",
    "    quantiles=quantiles,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
