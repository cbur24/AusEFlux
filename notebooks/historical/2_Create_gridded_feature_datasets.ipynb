{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create gridded feature datasets\n",
    "\n",
    "The goal here is to take the spatio-temporally harmonised files in `interim/` that were output from `1_Spatiotemporal_harmonization.ipynb` and stack them into ready-to-use netcdf files in the `5km` data folder. We want one netcdf per feature. Additionally, we create a few new features, for example vegetation fractions and climate anomalies etc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import rioxarray as rxr\n",
    "import numpy as np\n",
    "from odc.geo.xr import assign_crs\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/os22/chad_tmp/AusEFlux/data/interim/'\n",
    "results='/g/data/os22/chad_tmp/AusEFlux/data/5km/'\n",
    "\n",
    "#any datasets in interim we want to exclude?\n",
    "exclude = ['.ipynb_checkpoints', 'kTavg', 'Tmax', 'Tmin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loop through folders and join interim files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "folders = [i for i in os.listdir(base) if i not in exclude]\n",
    "folders.sort()\n",
    "\n",
    "for f in folders:\n",
    "    \n",
    "    if os.path.exists(results+f+'_5km_2003_2022.nc'):\n",
    "        print('','skipping '+f)\n",
    "        continue\n",
    "    else:\n",
    "        print(f)\n",
    "    \n",
    "    files = [f'{base}{f}/{i}' for i in os.listdir(base+f) if i.endswith(\".nc\")]\n",
    "    files.sort()\n",
    "\n",
    "    #combine annual files into one file\n",
    "    ds = xr.open_mfdataset(files)\n",
    "    ds = ds.chunk(dict(time=-1, latitude=250, longitude=250))\n",
    "                  \n",
    "    # Gapfill NDWI differently (has real gaps)\n",
    "    if f=='NDWI':\n",
    "        # seperate into climatologies and anomalies\n",
    "        ds_monthly = ds.groupby('time.month').mean()\n",
    "        ds_anom = ds.groupby('time.month') - ds_monthly  \n",
    "        \n",
    "        # fill linearly by max 2 steps\n",
    "        ds_anom = ds_anom.interpolate_na(dim='time', method='linear', limit=2)\n",
    "        \n",
    "        #recombine anomalies and climatology\n",
    "        ds = ds_anom.groupby('time.month') + ds_monthly\n",
    "        ds = ds.drop('month')\n",
    "        \n",
    "        #fill remaining gaps with climatology\n",
    "        ds = ds.groupby(\"time.month\").fillna(ds_monthly)\n",
    "\n",
    "    # ensure no gaps in other datasets (there shouldn't be any)\n",
    "    # this is just to be cautious\n",
    "    else:\n",
    "        ds_monthly = ds.groupby('time.month').mean()\n",
    "        ds = ds.groupby(\"time.month\").fillna(ds_monthly)\n",
    "    \n",
    "    ds = ds.drop('month').compute()\n",
    "\n",
    "    ds.to_netcdf(results+f+'_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create new features\n",
    "\n",
    "Using the climate and remote-sensing features we've already collected, we can generate new features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vegetation fractions (Trees, Grass, Bare)\n",
    "\n",
    "Following [Donohue 2009](https://onlinelibrary.wiley.com/doi/full/10.1111/j.1365-2486.2008.01746.x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NDVI value of bare soil (supplied by Luigi Renzullo)\n",
    "ndvi_min = xr.open_dataarray('/g/data/os22/chad_tmp/AusEFlux/data/ndvi_of_baresoil_5km.nc',\n",
    "                            chunks=dict(latitude=250, longitude=250)\n",
    "                            )\n",
    "\n",
    "# likely maximum possible NDVI\n",
    "ndvi_max = 0.91\n",
    "\n",
    "#ndvi data is here\n",
    "ndvi_path = '/g/data/os22/chad_tmp/AusEFlux/data/5km/NDVI_5km_2003_2022.nc'\n",
    "ds = xr.open_dataarray(ndvi_path, chunks=dict(time=-1, latitude=250, longitude=250))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#calculate f-total\n",
    "ft = (ds - ndvi_min) / (ndvi_max - ndvi_min)\n",
    "ft = xr.where(ft<0, 0, ft)\n",
    "ft = xr.where(ft>1, 1, ft)\n",
    "\n",
    "#calculate initial persistent fraction (equation 1 & 2 in Donohue 2009)\n",
    "persist = ft.rolling(time=7, min_periods=1).min()\n",
    "persist = persist.rolling(time=9, min_periods=1).mean()\n",
    "\n",
    "#calculate initial recurrent fraction (equation 3 in Donohue 2009)\n",
    "recurrent = ft - persist\n",
    "\n",
    "###------- equations 4 & 5 in Donohue 2009----------------\n",
    "persist = xr.where(recurrent<0, persist - np.abs(recurrent), persist) #eq4\n",
    "recurrent = ft - persist # eq 5\n",
    "## ---------------------------------------------------------\n",
    "\n",
    "#ensure values are between 0 and 1\n",
    "persist = xr.where(persist<0, 0, persist)\n",
    "recurrent = xr.where(recurrent<0, 0, recurrent)\n",
    "\n",
    "#assign variable names\n",
    "recurrent.name='grass'\n",
    "persist.name='trees'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Aggregate to annual layers\n",
    "\n",
    "Use the maximum fraction of trees and grass to create annual layers.\n",
    "\n",
    "Bare soil is the residual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "persist_annual = persist.resample(time='1Y').max().compute()\n",
    "recurrent_annual = recurrent.resample(time='1Y').max().compute()\n",
    "bare_annual = 1-(persist_annual+recurrent_annual)\n",
    "bare_annual.name='bare'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create monthly timeseries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dss_trees=[]\n",
    "dss_grass=[]\n",
    "dss_bare=[]\n",
    "for y in bare_annual.time.dt.year.values:\n",
    "    # print(y)\n",
    "    y = str(y)\n",
    "    time = pd.date_range(y+\"-01\", y+\"-12\", freq='MS') \n",
    "    time = [t+pd.Timedelta(14, 'd') for t in time]\n",
    "\n",
    "    #trees\n",
    "    ds_persist = persist_annual.sel(time=y).squeeze().drop('time')\n",
    "    ds_persist = ds_persist.expand_dims(time=time)\n",
    "    dss_trees.append(ds_persist)\n",
    "\n",
    "    #grass\n",
    "    ds_recurrent = recurrent_annual.sel(time=y).squeeze().drop('time')\n",
    "    ds_recurrent = ds_recurrent.expand_dims(time=time)\n",
    "    dss_grass.append(ds_recurrent)\n",
    "\n",
    "    ds_bare = bare_annual.sel(time=y).squeeze().drop('time')\n",
    "    ds_bare = ds_bare.expand_dims(time=time)\n",
    "    dss_bare.append(ds_bare)\n",
    "\n",
    "# join all the datasets back together\n",
    "trees = xr.concat(dss_trees, dim='time').sortby('time')\n",
    "grass = xr.concat(dss_grass, dim='time').sortby('time')\n",
    "bare = xr.concat(dss_bare, dim='time').sortby('time')\n",
    "\n",
    "# add right metadata\n",
    "trees.attrs['nodata'] = np.nan\n",
    "grass.attrs['nodata'] = np.nan\n",
    "bare.attrs['nodata'] = np.nan\n",
    "\n",
    "trees = assign_crs(trees, crs='EPSG:4326')\n",
    "grass = assign_crs(grass, crs='EPSG:4326')\n",
    "bare = assign_crs(bare, crs='EPSG:4326')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Export data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trees.to_netcdf(results+'trees_5km_2003_2022.nc')\n",
    "grass.to_netcdf(results+'grass_5km_2003_2022.nc')\n",
    "bare.to_netcdf(results+'bare_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cumulative rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain = xr.open_dataarray(results+'rain_5km_2003_2022.nc',chunks=dict(latitude=300, longitude=300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_cml_3 = rain.rolling(time=3, min_periods=3).sum()\n",
    "rain_cml_3 = rain_cml_3.rename('rain_cml3').sel(time=slice('2003','2022'))\n",
    "\n",
    "rain_cml_6 = rain.rolling(time=6, min_periods=6).sum()\n",
    "rain_cml_6 = rain_cml_6.rename('rain_cml6').sel(time=slice('2003','2022'))\n",
    "\n",
    "rain_cml_12 = rain.rolling(time=12, min_periods=12).sum()\n",
    "rain_cml_12 = rain_cml_12.rename('rain_cml12').sel(time=slice('2003','2022'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rain_cml_3.compute().to_netcdf(results+'rain_cml3_5km_2003_2022.nc')\n",
    "rain_cml_6.compute().to_netcdf(results+'rain_cml6_5km_2003_2022.nc')\n",
    "rain_cml_12.compute().to_netcdf(results+'rain_cml12_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rain_cml_12.mean(['latitude', 'longitude']).plot(figsize=(11,4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fractional climate anomalies\n",
    "\n",
    "fractional anomalies = obs / climatology (climatology goes from 2003-2022)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vars for which we will calculate frcational anomalies\n",
    "vars = ['NDVI', 'rain', 'rain_cml3', 'rain_cml6', 'rain_cml12', 'SRAD', 'Tavg', 'VPD']\n",
    "\n",
    "for v in vars:\n",
    "    print(v)\n",
    "    ds = assign_crs(xr.open_dataset(results+v+'_5km_2003_2022.nc'), crs='EPSG:4326')\n",
    "    mean = ds.groupby(\"time.month\").mean(\"time\")\n",
    "    frac = ds.groupby(\"time.month\") / mean\n",
    "    frac.drop('month').rename({v:v+'_anom'}).to_netcdf(results+v+'_anom_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LST - Tair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tair = xr.open_dataarray(results+'Tavg_5km_2003_2022.nc')\n",
    "\n",
    "lst = xr.open_dataarray(results+'LST_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaT = (lst-273.15) - tair\n",
    "deltaT.name = u'ΔT'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deltaT.to_netcdf(results+u'ΔT_5km_2003_2022.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
