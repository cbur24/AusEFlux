{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Develop a method for harmonizing data from OzWALD\n",
    "\n",
    "Data is here: https://dapds00.nci.org.au/thredds/catalog/ub8/au/catalog.html\n",
    "\n",
    "Requirements:\n",
    "* Must be reproducible in an operational context i.e. minumum of fuss to rerun the whole process each year, but first off we need a ~20yr archive to build the models and run historic predictions\n",
    "* For now, run at 5 km resolution\n",
    "* Intermediate files are fine, but lets keep the number of steps to a minimum\n",
    "* Some variables are already computed by OzWALD, but others need to be either computed on-the-fly or saved and stored as intermediate files.\n",
    "* Many of the pre-computed variables available in OzWALD require resampling spatially and temporally\n",
    "* A python environment is required, but should be a small as possible (but will undoubtedly still be cumbersome)\n",
    "* There is a soft requirement that the model be built on features as close to possible as the published 'AusEFlux' article in Biogeosciences.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "from odc.geo.xr import assign_crs\n",
    "from odc.geo.geobox import zoom_out\n",
    "\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _utils import start_local_dask, round_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = '/g/data/ub8/au/'\n",
    "results='/g/data/os22/chad_tmp/AusEFlux/data/interim/'\n",
    "years = [str(i) for i in range(2003,2023)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grab a common grid to reproject too and a create a land mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbox = xr.open_dataset('/g/data/os22/chad_tmp/climate-carbon-interactions/data/5km/WCF_5km_monthly_1982_2022.nc').odc.geobox\n",
    "gbox\n",
    "\n",
    "#create a mask of aus extent\n",
    "mask = xr.open_dataset('/g/data/os22/chad_tmp/climate-carbon-interactions/data/5km/WCF_5km_monthly_1982_2022.nc')['WCF']\n",
    "mask = mask.mean('time')\n",
    "mask = xr.where(mask>-99, 1, 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIS NDWI (vegetation water)\n",
    "\n",
    "The orginal Gao (1996) paper says to use the 1230_1250nm band (band 5 in MODIS), but other sources suggest band 6. On the basis of experiments, looks like B6 is more sensitive.\n",
    "\n",
    "This takes ~9-10 mins per year to process using 24 cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=dict(latitude=1000, longitude=1000, time=1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "    if os.path.exists(results+'NDWI'+'/NDWI_5km_'+year+'.nc'):\n",
    "            continue\n",
    "    else:\n",
    "        print('NDWI', year)\n",
    "    \n",
    "    modis_sr_inputs = {\n",
    "        'SR_B2': 'MODIS/mosaic/MCD43A4.006/MCD43A4.006.b02.500m_0841_0876nm_nbar.'+year+'.nc',\n",
    "        'SR_B6': 'MODIS/mosaic/MCD43A4.006/MCD43A4.006.b06.500m_1628_1652nm_nbar.'+year+'.nc',\n",
    "        # 'SR_B5': 'MODIS/mosaic/MCD43A4.006/MCD43A4.006.b05.500m_1230_1250nm_nbar.'+year+'.nc',\n",
    "        \n",
    "         }\n",
    "\n",
    "    d = {}\n",
    "    for k,i in modis_sr_inputs.items():\n",
    "        \n",
    "        #open and do some prelim processing\n",
    "        ds = xr.open_dataset(base+i, chunks=chunks)\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop_vars('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        ds = ds.rename(k)        \n",
    "        d[k] = ds #add to dict\n",
    "    \n",
    "    #calculate NDWI (water index)\n",
    "    ndwi = (d['SR_B2'] - d['SR_B6']) / (d['SR_B2'] + d['SR_B6'])\n",
    "\n",
    "    #resample time\n",
    "    ndwi = ndwi.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean().persist()\n",
    "\n",
    "    # resample spatial\n",
    "    ndwi = ndwi.odc.reproject(gbox, resampling='average').compute()  # bring into memory\n",
    "    \n",
    "    #tidy up\n",
    "    ndwi = round_coords(ndwi)\n",
    "    ndwi.attrs['nodata'] = np.nan\n",
    "    ndwi = ndwi.rename('WI')\n",
    "\n",
    "    #mask to aus land extent\n",
    "    ndwi = ndwi.where(mask)\n",
    "    \n",
    "    #export result\n",
    "    folder = '/g/data/os22/chad_tmp/AusEFlux/data/interim/WI'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    ndwi.astype('float32').to_netcdf(results+'NDWI'+'/NDWI_5km_'+year+'.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MODIS LST\n",
    "\n",
    "Using Aqua land surface temperature (afternoon overpass) i.e. `MYD11A1.006`\n",
    "\n",
    "This takes ~5 mins per year to process using 24 cores\n",
    "\n",
    "QC masking MODIS, help: https://spatialthoughts.com/2021/08/19/qa-bands-bitmasks-gee/\n",
    "\n",
    "decimal to binary converter: https://www.rapidtables.com/convert/number/decimal-to-binary.html\n",
    "\n",
    "\n",
    "- 0  (decimal): 0 0 0 0 0 0 0 0; LST produced, good quality, good data, emis err < 0.01, LST error <1K\n",
    "- 5  (decimal): 0 0 0 0 0 1 0 1; LST produced, other quality, other quality, emis err < 0.01, LST error <= 1K\n",
    "- 17 (decimal): 0 0 0 1 0 0 0 1; LST produced, other quality, good data, emis err < 0.02, LST error <= 1K\n",
    "- 21 (decimal): 0 0 0 1 0 1 0 1; LST produced, other quality, other quality, emis err < 0.02, LST error <= 1K\n",
    "- 64 (decimal): 0 1 0 0 0 0 0 0; LST produced, good quality, good data, emis err < 0.01, LST error <= 2K\n",
    "- 65 (decimal): 0 1 0 0 0 0 0 1; LST produced, other quality, good data, emis err < 0.01, LST error <= 2K\n",
    "- 81 (decimal): 0 1 0 1 0 0 0 1; LST produced, other quality, other quality, emis err < 0.02, LST error <= 2K\n",
    "< cores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(latitude=500, longitude=500, time=-1)\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "    modis_sr_inputs = {\n",
    "        'LST' :'MODIS/mosaic/MYD11A1.006/MYD11A1.006.LST_Day_1km.'+year+'.nc'\n",
    "         }\n",
    "    \n",
    "    for k,i in modis_sr_inputs.items():\n",
    "         \n",
    "        if os.path.exists(results+'LST'+'/LST_5km_'+year+'.nc'):\n",
    "            continue\n",
    "        else:\n",
    "            print(k, year)\n",
    "    \n",
    "        ds = xr.open_dataset(base+i,chunks=chunks)\n",
    "\n",
    "        #deal with messed up QC in 2022 (temporary hopefully)\n",
    "        if year != '2022':\n",
    "            qc = xr.open_dataset(base+'MODIS/mosaic/MYD11A1.006/MYD11A1.006.QC_Day.'+year+'.nc',\n",
    "                     chunks=chunks)\n",
    "            #data is high quality <2k error see above.\n",
    "            m = xr.where((qc.QC_Day==0) | (qc.QC_Day==5) | (qc.QC_Day==17) | (qc.QC_Day==21) |\n",
    "                         (qc.QC_Day==64) | (qc.QC_Day==65) | (qc.QC_Day==81), 1, 0)\n",
    "            \n",
    "            ds = ds.where(m)\n",
    "\n",
    "        #tidy up\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "\n",
    "        #resample time and space\n",
    "        ds = ds.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean().compute()\n",
    "        ds = ds.odc.reproject(gbox, resampling='average')\n",
    "\n",
    "        #tidy up\n",
    "        ds = round_coords(ds)\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        ds = ds.rename(k)\n",
    "        ds = ds.where(mask) #land mask\n",
    "\n",
    "        #convert to celsius\n",
    "        ds = ds-273.15\n",
    "        \n",
    "        #export result\n",
    "        folder = results+k\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        ds.to_netcdf(results+k+'/'+k+'_5km_'+year+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## OzWALD NDVI & EVI (derived from MODIS)\n",
    "\n",
    "This takes ~1.5 mins per year to process using 24 cores\n",
    "\n",
    "NDVI is used derive the vegetation fractions, EVI is used as the predictor in ML model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(latitude=1000, longitude=1000, time=-1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "    modis_ndvi = {\n",
    "        'NDVI' :'OzWALD/8day/NDVI/OzWALD.NDVI.'+year+'.nc',\n",
    "        'EVI' :'OzWALD/8day/EVI/OzWALD.EVI.'+year+'.nc'\n",
    "         }\n",
    "    \n",
    "    for k,i in modis_ndvi.items():\n",
    "         \n",
    "        if os.path.exists(f'{results}{k}/{k}_5km_{year}.nc'):\n",
    "            continue\n",
    "        else:\n",
    "            print(k, year)\n",
    "        \n",
    "        ds = xr.open_dataset(base+i,chunks=chunks)\n",
    "        ds = ds.transpose('time', 'latitude', 'longitude')\n",
    "        \n",
    "        #tidy up\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        \n",
    "        #resample time and space\n",
    "        ds = ds.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean().compute()\n",
    "        ds = ds.odc.reproject(gbox, resampling='average')\n",
    "        \n",
    "        #tidy up\n",
    "        ds = round_coords(ds)\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        ds = ds.rename(k)\n",
    "        ds = ds.where(mask) #land mask\n",
    "\n",
    "        #export result\n",
    "        folder = results+k\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        ds.to_netcdf(f'{results}{k}/{k}_5km_{year}.nc')\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(latitude=1000, longitude=1000, time=-1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "    modis_ndvi = {\n",
    "        'NDVI' :'OzWALD/8day/NDVI/OzWALD.NDVI.'+year+'.nc'\n",
    "         }\n",
    "    \n",
    "    for k,i in modis_ndvi.items():\n",
    "         \n",
    "        if os.path.exists(results+'NDVI'+'/NDVI_5km_'+year+'.nc'):\n",
    "            continue\n",
    "        else:\n",
    "            print(k, year)\n",
    "        \n",
    "        ds = xr.open_dataset(base+i,chunks=chunks)\n",
    "        ds = ds.transpose('time', 'latitude', 'longitude')\n",
    "        \n",
    "        #tidy up\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        \n",
    "        #resample time and space\n",
    "        ds = ds.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean().compute()\n",
    "        ds = ds.odc.reproject(gbox, resampling='average')\n",
    "        \n",
    "        #tidy up\n",
    "        ds = round_coords(ds)\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        ds = ds.rename(k)\n",
    "        ds = ds.where(mask) #land mask\n",
    "\n",
    "        #export result\n",
    "        folder = results+k\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        ds.to_netcdf(results+k+'/'+k+'_5km_'+year+'.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vegetation Height\n",
    "\n",
    "This was reprojected from 25m to 1 km previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset('/g/data/os22/chad_tmp/AusEFlux/data/VegH_1km_2007_2010.nc', chunks=dict(x=250, y=250))\n",
    "ds = assign_crs(ds, crs='epsg:4326')\n",
    "ds.attrs['nodata'] = np.nan\n",
    "ds = ds['VegH']\n",
    "ds = ds.odc.reproject(gbox, resampling='average').compute()\n",
    "ds = round_coords(ds)\n",
    "\n",
    "# convert to time-series (same values for each time-step)\n",
    "# create a dataset for each year just to be consistent with other features\n",
    "# open another dataset so we can grab the time dim\n",
    "for year in years:\n",
    "    \n",
    "    if os.path.exists(results+'VegH/VegH_5km_'+year+'.nc'):\n",
    "            continue\n",
    "    else:\n",
    "        print(year)\n",
    "            \n",
    "    da = xr.open_dataarray(results+'NDWI'+'/NDWI_5km_'+year+'.nc')\n",
    "    \n",
    "    #expand time dim using other dataset's time.\n",
    "    dss = ds.expand_dims(time=da.time)\n",
    "    \n",
    "    #mask to aus land extent\n",
    "    dss = dss.where(mask)\n",
    "    dss = dss.rename('VegH')\n",
    "    #export\n",
    "    dss.to_netcdf(results+'VegH/VegH_5km_'+year+'.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Climate data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rain\n",
    "\n",
    "This runs very fast, ~6 seconds per year using 24 cores\n",
    "\n",
    "With rainfall, we need to grab data from a year earlier (from 2002 onwards) because later on we calculate 12-month cumulative rainfall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chunks=dict(latitude=250, longitude=250, time=-1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "\n",
    "    clim_inputs = {\n",
    "        'rain': 'OzWALD/daily/meteo/Pg/OzWALD.daily.Pg.'+year+'.nc'\n",
    "         }\n",
    "    \n",
    "    d = {}\n",
    "    for k,i in clim_inputs.items():\n",
    "        \n",
    "        #open and do some prelim processing\n",
    "        ds = xr.open_dataset(base+i, chunks=chunks).persist()\n",
    "        ds = ds.transpose('time', 'latitude', 'longitude')\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop_vars('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        ds = ds.rename(k)        \n",
    "        d[k] = ds #add to dict\n",
    "            \n",
    "    if os.path.exists(f'{results}/{k}/{k}_5km_{year}.nc'):\n",
    "        continue\n",
    "    else:\n",
    "        print(k, year)\n",
    "            \n",
    "    #resample time, bring into memory\n",
    "    ds = d['rain'].resample(time='MS', loffset=pd.Timedelta(14, 'd')).sum().compute()\n",
    "\n",
    "    # resample spatial\n",
    "    ds = ds.odc.reproject(gbox, resampling='nearest')\n",
    "    \n",
    "    #tidy up\n",
    "    ds = round_coords(ds)\n",
    "    ds.attrs['nodata'] = np.nan\n",
    "    ds = ds.rename(k)\n",
    "\n",
    "    #mask to aus land extent\n",
    "    ds = ds.where(mask)\n",
    "\n",
    "    #export result\n",
    "    folder = results+k\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    ds.astype('float32').to_netcdf(f'{results}/{k}/{k}_5km_{year}.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tavg\n",
    "\n",
    "The temperature datasets are daily and 500m resolution, Tavg can only be calculated as `Tavg = Tmin + kTavg*(Tmax - Tmin)` and each variable is stored as one large netcdf per year on file (this limits parallelization in loading the datasets) so the memory and compute requirements are v. large.  \n",
    "\n",
    "This process is so slow we need to break into up into two steps.\n",
    "1. Iteratively load each of the three variables we need to calculate Tavg, reproject them to 5km resolution and save to disk\n",
    "2. Calculate Tavg and save to disk\n",
    "\n",
    "Step 1 takes ~ 87 mins per year to run using 24 cores.\n",
    "\n",
    "Step 2. takes ~ 10-20 seconds per year"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(latitude=10000, longitude=10000, time=1) #ie one chunk per time\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "\n",
    "    clim_inputs = {\n",
    "        'Tmin':'OzWALD/daily/meteo/Tmin/OzWALD.Tmin.'+year+'.nc', \n",
    "        'Tmax':'OzWALD/daily/meteo/Tmax/OzWALD.Tmax.'+year+'.nc',\n",
    "        'kTavg':'OzWALD/daily/meteo/kTavg/OzWALD.kTavg.'+year+'.nc'\n",
    "         }\n",
    "    \n",
    "    for k,i in clim_inputs.items():\n",
    "        \n",
    "        if os.path.exists(f'{results}/{k}/{k}_5km_{year}.nc'):\n",
    "            continue\n",
    "        else:\n",
    "            print(k, year)\n",
    "        \n",
    "        #open and do some prelim processing\n",
    "        ds = xr.open_dataset(base+i, chunks=chunks) # open as one chunk per time\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        ds = ds.to_array()\n",
    "        ds = ds.squeeze().drop_vars('variable')\n",
    "        ds.attrs['nodata'] = np.nan\n",
    "        #ds = ds.chunk(latitude=10000, longitude=10000, time=1) # now rechunk for the reproject\n",
    "        \n",
    "        #we need to spatial resample first to reduce RAM/speed up.\n",
    "        if k=='kTavg':\n",
    "            #upscaling from 10km to 5km\n",
    "            ds = ds.odc.reproject(gbox, resampling='nearest').compute()\n",
    "            ds = round_coords(ds)\n",
    "        else:\n",
    "            # downsacling from 500m to 5km\n",
    "            ds = ds.odc.reproject(gbox, resampling='average').compute()\n",
    "            ds = round_coords(ds)\n",
    "\n",
    "        #tidy up\n",
    "        ds = ds.transpose('time', 'latitude', 'longitude')\n",
    "        ds = ds.rename(k)\n",
    "        ds = assign_crs(ds, crs='epsg:4326')\n",
    "        \n",
    "        # #export result\n",
    "        folder = results+k\n",
    "        if not os.path.exists(folder):\n",
    "            os.makedirs(folder)\n",
    "        \n",
    "        ds.astype('float32').to_netcdf(f'{results}/{k}/{k}_5km_{year}.nc')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2\n",
    "\n",
    "no need for dask now"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "#loop through each year\n",
    "for year in years:\n",
    "\n",
    "    clim_inputs = {\n",
    "        'Tmin':f'{results}/Tmin/Tmin_5km_{year}.nc', \n",
    "        'Tmax':f'{results}/Tmax/Tmax_5km_{year}.nc',\n",
    "        'kTavg':f'{results}/kTavg/kTavg_5km_{year}.nc'\n",
    "         }\n",
    "    \n",
    "    if os.path.exists(f'{results}/Tavg/Tavg_5km_{year}.nc'):\n",
    "            continue\n",
    "    else:\n",
    "        print('Tavg', year)\n",
    "    \n",
    "    d={}\n",
    "    for k,i in clim_inputs.items():\n",
    "        ds = xr.open_dataarray(i)\n",
    "        d[k] = ds\n",
    "    \n",
    "    #calculate tavg\n",
    "    ds = d['Tmin'] + d['kTavg']*(d['Tmax'] - d['Tmin'])\n",
    "\n",
    "    #resample time\n",
    "    ds = ds.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean()\n",
    "    \n",
    "    #tidy up\n",
    "    ds.attrs['nodata'] = np.nan\n",
    "    ds = ds.rename('Tavg')\n",
    "\n",
    "    #mask to aus land extent\n",
    "    ds = ds.where(mask)\n",
    "\n",
    "    #export result\n",
    "    folder = '/g/data/os22/chad_tmp/AusEFlux/data/interim/Tavg/'\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    ds.astype('float32').to_netcdf(f'{results}/Tavg/Tavg_5km_{year}.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solar radiation\n",
    "\n",
    "Incoming shortwave rad (MJ/m2/d), using SILO version https://dapds00.nci.org.au/thredds/catalog/ub8/au/SILO/radiation/catalog.html\n",
    "\n",
    "Can account for the effect of topography on incoming radiation in the same way as OzWALD, where it is done by multiplying incoming radiation with the grids:`\r\n",
    "//g/data/xc0/project/OzWALD/R2021/model/static/SWratio_500m_*.`nc grids. Where the wild card is the month of the year from 01:12 \r",
    "- This is fast, 5 seconds per year\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(lat=250, lon=250, time=-1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "        clim_inputs = {\n",
    "            'SRAD':'SILO/radiation/'+year+'.radiation.nc'\n",
    "             }\n",
    "        \n",
    "        for k,i in clim_inputs.items():\n",
    "            \n",
    "            if os.path.exists(f'{results}/{k}/{k}_5km_{year}.nc'):\n",
    "                continue\n",
    "            else:\n",
    "                print(k, year)\n",
    "            \n",
    "            #open and do some prelim processing\n",
    "            ds = xr.open_dataset(base+i).drop('crs').chunk(chunks)\n",
    "            ds = assign_crs(ds, crs='epsg:4326')\n",
    "            ds = ds.to_array()\n",
    "            ds = ds.squeeze().drop_vars('variable')\n",
    "            ds.attrs['nodata'] = np.nan\n",
    "    \n",
    "            # resample time and space\n",
    "            ds = ds.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean()\n",
    "            ds = ds.odc.reproject(gbox, resampling='nearest').compute()\n",
    "            ds = round_coords(ds)\n",
    "            \n",
    "            # tidy up and mask land\n",
    "            ds = ds.rename(k)\n",
    "            ds = assign_crs(ds, crs='epsg:4326')\n",
    "            ds = ds.where(mask)\n",
    "    \n",
    "            # # #export result\n",
    "            folder = results+k\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            \n",
    "            ds.astype('float32').to_netcdf(f'{results}/{k}/{k}_5km_{year}.nc')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vapour Pressure Deficit\n",
    "\n",
    "Using SILO version for VP: https://dapds00.nci.org.au/thredds/catalog/ub8/au/SILO/vp/catalog.html\n",
    "\n",
    "Calculating VPD requires air temperature, so this must be run after Tavg has been computed.\n",
    " \r",
    "- This is fast, 5 seconds per year\n",
    "\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "chunks=dict(lat=250, lon=250, time=-1)\n",
    "\n",
    "#loop through each year\n",
    "for year in years:\n",
    "    \n",
    "        clim_inputs = {\n",
    "            'VPD':'SILO/vp/'+year+'.vp.nc'\n",
    "             }\n",
    "        \n",
    "        for k,i in clim_inputs.items():\n",
    "            \n",
    "            if os.path.exists(f'{results}/{k}/{k}_5km_{year}.nc'):\n",
    "                continue\n",
    "            else:\n",
    "                print(k, year)\n",
    "            \n",
    "            #open and do some prelim processing\n",
    "            vp = xr.open_dataset(base+i).drop('crs').chunk(chunks)\n",
    "            vp = assign_crs(vp, crs='epsg:4326')\n",
    "            vp = vp.to_array()\n",
    "            vp = vp.squeeze().drop_vars('variable')\n",
    "            vp.attrs['nodata'] = np.nan\n",
    "    \n",
    "            # resample time and space\n",
    "            vp = vp.resample(time='MS', loffset=pd.Timedelta(14, 'd')).mean()\n",
    "            vp = vp.odc.reproject(gbox, resampling='nearest').compute()\n",
    "            vp = round_coords(vp)\n",
    "            \n",
    "            # mask land\n",
    "            vp = assign_crs(vp, crs='epsg:4326')\n",
    "            vp = vp.where(mask)\n",
    "\n",
    "            #calculate VPD\n",
    "            ta = xr.open_dataarray(f'{results}/Tavg/Tavg_5km_{year}.nc')\n",
    "            sat_vp = (6.11 * np.exp((2500000/461) * (1/273 - 1/(273 + ta))))\n",
    "            ds = sat_vp - vp\n",
    "\n",
    "            # tidy up\n",
    "            ds = ds.rename(k)\n",
    "            ds.attrs['nodata'] = np.nan\n",
    "            ds.attrs['units'] = 'hPa'\n",
    "    \n",
    "            # export result\n",
    "            folder = results+k\n",
    "            if not os.path.exists(folder):\n",
    "                os.makedirs(folder)\n",
    "            \n",
    "            ds.astype('float32').to_netcdf(f'{results}/{k}/{k}_5km_{year}.nc')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
