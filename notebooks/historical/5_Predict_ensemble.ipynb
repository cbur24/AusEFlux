{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate an ensemble of gridded predictions\n",
    "\n",
    "Using the models produced in `4_Generate_ensemble_of_models.ipynb`, we will generate an ensemble of predictions. From this ensemble we will produce an uncertainty envelope, and a median prediction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import warnings\n",
    "import xarray as xr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from joblib import load\n",
    "from matplotlib import pyplot as plt\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "sys.path.append('/g/data/os22/chad_tmp/AusEFlux/src/')\n",
    "from _prediction import collect_prediction_data, predict_xr, HiddenPrints\n",
    "from _utils import start_local_dask, round_coords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_var = 'NEE'\n",
    "base = '/g/data/os22/chad_tmp/AusEFlux/'\n",
    "\n",
    "results_path = f'{base}results/predictions/ensemble/{model_var}/'\n",
    "models_folder = f'{base}results/models/ensemble/{model_var}/'\n",
    "features_list = f'{base}results/variables.txt'\n",
    "\n",
    "t1, t2='2003','2022'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get paths to models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_list = [file for file in os.listdir(models_folder) if file.endswith(\".joblib\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Open predictor data\n",
    "\n",
    "At 1 km resolution, we need to pull the gridded feature layers in as dask arrays and compute on each time-step individually as the total memory requirements are very large. At 5 km resolution, its better to load the entire feature layer data into memory as it speeds up predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## open data\n",
    "data = collect_prediction_data(data_path=f'{base}/data/5km/',\n",
    "                             time_range=(t1,t2),\n",
    "                             verbose=False,\n",
    "                             export=False,\n",
    "                             chunks=dict(time=1)\n",
    "                             )\n",
    "\n",
    "# data = data.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create no-data & urban masks\n",
    "\n",
    "If we haven't already"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mask = data[['EVI', 'NDWI', 'VegH', 'SRAD']].to_array().isnull().any('variable')\n",
    "# mask.compute().to_netcdf(f'{base}data/mask_5km_monthly_{t1}_{t2}.nc')\n",
    "\n",
    "# #create an urban mask once, then next time load it.\n",
    "# urban = xr.open_dataarray('/g/data/os22/chad_tmp/NEE_modelling/data/urban_mask_1km.nc')\n",
    "# urban = urban.odc.reproject(mask.odc.geobox, resampling='mode')\n",
    "# urban=round_coords(urban)\n",
    "# urban.name='urban_mask'\n",
    "# urban = urban.astype(bool).rename({'latitude':'y', 'longitude':'x'})\n",
    "# urban.compute().to_netcdf(f'{base}data/urban_mask_5km.nc')\n",
    "\n",
    "#open the mask if already created.\n",
    "mask = xr.open_dataarray(f'{base}data/mask_5km_monthly_{t1}_{t2}.nc')\n",
    "urban = xr.open_dataset(f'{base}data/urban_mask_5km.nc')['urban_mask']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Index by variables and check variable order"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_vars = list(pd.read_csv(features_list))[0:-1]\n",
    "train_vars.remove('site')\n",
    "train_vars=[i[:-3] for i in train_vars]\n",
    "\n",
    "data = data[train_vars]\n",
    "\n",
    "if train_vars == list(data.data_vars):\n",
    "    print('Variables match, n:', len(data.data_vars))\n",
    "else:\n",
    "    raise ValueError(\"Variables don't match\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict\n",
    "\n",
    "Loop through each model, and each time-step.  Mask the output with the urban mask.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through models\n",
    "for m in model_list:\n",
    "    name = m.split('.')[0]\n",
    "    \n",
    "    if os.path.exists(f'{results_path}{name}.nc'):\n",
    "        print('skipping model '+name)\n",
    "        continue\n",
    "    \n",
    "    print('Model: ', name)\n",
    "    \n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "    model = load(models_folder+m).set_params(n_jobs=1)\n",
    "    \n",
    "    results = []\n",
    "    i=0\n",
    "    #loop through the time-steps\n",
    "    for i in range(0, len(data.time)): \n",
    "        print(\"  {:03}/{:03}\\r\".format(i + 1, len(range(0, len(data.time)))), end=\"\")\n",
    "\n",
    "        with HiddenPrints():\n",
    "            warnings.filterwarnings(\"ignore\")\n",
    "            predicted = predict_xr(model,\n",
    "                                data.isel(time=i),\n",
    "                                proba=False,\n",
    "                                clean=True,\n",
    "                                #chunk_size=875000, #this number is optimized to maximise pred speed.\n",
    "                                  ).compute()\n",
    "\n",
    "                #mask no-data areas\n",
    "            predicted = predicted.Predictions.where(~mask.isel(time=i))\n",
    "        \n",
    "            #add back time dim\n",
    "            predicted['time'] = data.isel(time=i).time.values\n",
    "        \n",
    "            #append to list\n",
    "            results.append(predicted.astype('float32'))\n",
    "            i+=1 \n",
    "\n",
    "        #join together into a Dataset\n",
    "        ds = xr.concat(results, dim='time').sortby('time').rename(model_var).astype('float32')\n",
    "        \n",
    "        #mask urban areas\n",
    "        ds = ds.where(urban!=1).astype('float32')\n",
    "\n",
    "        #save results\n",
    "        ds.to_netcdf(f'{results_path}{name}.nc')\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
