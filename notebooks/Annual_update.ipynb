{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Annual updating of AusEFlux <img align=\"right\" src=\"https://github.com/cbur24/AusEFlux/blob/master/banner_picture.png?raw=True\" width=\"40%\">\n",
    "\n",
    "This notebook executes a workflow for annual updating of AusEFlux terrestrial carbon and water fluxes. It contains four main steps, instructions are provided in the subsections below. Pay close attention to the `Analysis Parameters` sections and ensure paths etc. are correct.\n",
    "\n",
    "***\n",
    "**Ideal compute environment:**\n",
    "\n",
    "Assuming 500m resolution. At this high resolution the process is quite slow.\n",
    "\n",
    "- NCI's 'hugemem' queue\n",
    "- X-large (24 cores, 765GiB)\n",
    "- Python 3.10.0\n",
    "- Python venv: `/g/data/xc0/project/AusEFlux/env/py310`\n",
    "- Storage Folders: `gdata/ub8+gdata/xc0`\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore')\n",
    "\n",
    "import sys\n",
    "sys.path.append('/g/data/xc0/project/AusEFlux/src/')\n",
    "from _utils import start_local_dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "client = start_local_dask(mem_safety_margin='2Gb')\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up project directory structure\n",
    "\n",
    "This workflow assumes a specific file/folder structure, here we create that folder structure to support the rest of the process.\n",
    "\n",
    "Below, enter the `root directory location` where project results and data are stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root='/g/data/xc0/project/AusEFlux'\n",
    "target_grid = '500m'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _utils import create_project_directories\n",
    "create_project_directories(root_dir=root, target_grid=target_grid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Spatiotemporal harmonisation of input datasets\n",
    "\n",
    "Most datasets are originally from here: https://thredds.nci.org.au/thredds/catalog/ub8/au/catalog.html\n",
    "\n",
    "Dataset from this process are output as annual layers in `data/interim`\n",
    "\n",
    "**Expected completion time at 500m ~3.5 hours**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `target_grid`: The spatial resolution of the product we are building\n",
    "* `results`: Path to store interim datasets after they have undergone harmonisatin\n",
    "* `year`: The year of data we are processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_grid = '500m'\n",
    "results=f'/g/data/xc0/project/AusEFlux/data/interim_{target_grid}/'\n",
    "year = 2024"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _harmonisation import spatiotemporal_harmonisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "spatiotemporal_harmonisation(\n",
    "    year_start=year,\n",
    "    year_end=year,\n",
    "    target_grid=target_grid,\n",
    "    results_path=results,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create feature datasets\n",
    "\n",
    "Combine results of the spatiotemporal harmonisation into temporally stacked netcdf files, and create new features/variables based on the climate (e.g. anomalies) and remote sensing (e.g. vegetation fractions) datasets. \n",
    "\n",
    "**Expected completion time at 500m resolution is ~6 hours**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `target_grid`: The spatial resolution of the product we are building\n",
    "* `base`: Path to where the harmonised datasets output from Step 1 are stored. \n",
    "* `results`: Path to store temporally stacked netcdf files i.e. where the outputs of Step 2 will be stored\n",
    "* `exclude`: Variables to exclude from combining. i.e. Some of the variables in `/interim` output in Step 1 are not needed hereafter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#results are stored in different directory due to storage issues\n",
    "target_grid = '500m'\n",
    "base = f'/g/data/xc0/project/AusEFlux/data/interim_{target_grid}/'\n",
    "results=f'/g/data/xc0/project/AusEFlux/data/{target_grid}/'\n",
    "exclude = ['.ipynb_checkpoints', 'kTavg', 'Tmax', 'Tmin']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 2\n",
    "\n",
    "start time = 1511"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from _feature_datasets import create_feature_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_datasets(\n",
    "    base=base,\n",
    "    results_path=results,\n",
    "    exclude=exclude,\n",
    "    target_grid=target_grid,\n",
    "    verbose=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Predict ensemble\n",
    "\n",
    "Using the ensemble of models, we will generate an ensemble of gridded predictions. The code below will loop through the carbon and water fluxes and submit each job to the gadi queue. If, for some reason, you need to adapt the parameters of the shell script that submits the jobs then it can be found at `/g/data/xc0/project/AusEFlux/src/_qsub_ensemble_member.sh`\n",
    "\n",
    "Its advisable that you check that all 30 ensemble members for each flux are exported, as it is not uncommon for a Gadi job to fail for no apparent reason and the results won't be exported (this is very likely when running 120 large jobs simultaneously). In which case, run the code block again as the code first checks if the result already exists.\n",
    "\n",
    "**Annual fluxes will begin to be exported within ~10-15 mins, but it depends on the compute available as to how long it will take to get through all 120 files.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "* `fluxes`: The list of fluxes to loop through and predict. Most times this shouldn't need to change.\n",
    "* `base`: Path to the root directory\n",
    "* `target_grid`: The spatial resolution of the product we are building\n",
    "* `year_start`: The first year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series to predict. If predicting for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `features_list`: Where are the list of features used by the model?\n",
    "* `prediction_data`: Where are the combined feature datasets stored?\n",
    "* `n_workers`: When we start the dask client, how many cores will the client have?\n",
    "* `memory_limit`: the amount of memory to assign to each dask client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = ['GPP','NEE','ER','ET']\n",
    "base = '/g/data/xc0/project/AusEFlux/'\n",
    "year_start, year_end=2024, 2024\n",
    "target_grid='500m'\n",
    "features_list = f'{base}results/variables.txt'\n",
    "prediction_data=f'{base}data/{target_grid}'\n",
    "n_workers=26\n",
    "memory_limit='140GiB'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Step 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "for f in fluxes:\n",
    "    # set up paths\n",
    "    results_path = f'{base}results/predictions/annual_update/{year_start}/{f}/'\n",
    "    models_folder = f'{base}results/models/ensemble/{f}/'\n",
    "    \n",
    "    #paths to models\n",
    "    model_list = [models_folder+file for file in os.listdir(models_folder) if file.endswith(\".joblib\")]\n",
    "    model_list.sort()\n",
    "    os.chdir('/g/data/xc0/project/AusEFlux/') #so o,e files get spit out here.\n",
    "    \n",
    "    #submit each model to gadi seperately for prediction\n",
    "    for m in model_list:\n",
    "        \n",
    "        name =  m.split('/')[-1].split('.')[0]\n",
    "\n",
    "        #check if its already been  predicted\n",
    "        if os.path.exists(f'{results_path}{name}.nc'):\n",
    "            pass\n",
    "        else:\n",
    "            #submit to Gadi\n",
    "            os.system(f\"qsub -v model_path={m},model_var={f},year_start={year_start},year_end={year_end},target_grid={target_grid},base={base},results_path={results_path},prediction_data={prediction_data},features_list={features_list},n_workers={n_workers},memory_limit={memory_limit} /g/data/xc0/project/AusEFlux/src/_qsub_ensemble_member.sh\"\n",
    "                     )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!qstat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Combine ensembles\n",
    "\n",
    "We ran an ensemble of predictions, now we need to compute the ensemble median and the uncertainty range.\n",
    "\n",
    "This step will also output the datasets with appropriate metadata.  All datasets will be exported to: \n",
    "`/g/data/xc0/project/AusEFlux/results/AusEFlux/<flux>/`\n",
    "\n",
    "The code below will loop through the carbon and water fluxes.\n",
    "\n",
    "**Expected completion time ~30 mins**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis Parameters\n",
    "\n",
    "* `fluxes`: A dictionary linking the fluxes to be modelled (e.g 'GPP', 'NEE' etc.) with their full names (e.g. 'Gross Primary Productivity'). This dictionary is used to loop through the fluxes for combining the ensembles, and the full name is used for metadata on the exported netcdf.\n",
    "* `predictions_path`: Where is the top level folder that the predictions will be exported too? \n",
    "* `year_start`: The first year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `year_end`: The last year in the series. If running for a single year, make _year_start_ and _year_end_ the same.\n",
    "* `target_grid`: The spatial resolution of the product we are building\n",
    "* `quantiles`: What quantiles are we using to determine the middle value and uncertainty range? The default is 0.25 and 0.75 for the uncertainty envelope, and 0.5 (median) for the middle estimate.\n",
    "* `version`: What version of the dataset is this?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fluxes = {\n",
    "    'GPP':'Gross Primary Productivity',\n",
    "    'NEE':'Net Ecosystem Exchange',\n",
    "    'ER':'Ecosystem Respiration',\n",
    "    'ET':'Evapotranspiration'\n",
    "         }\n",
    "\n",
    "base = '/g/data/xc0/project/AusEFlux/'\n",
    "predictions_path = f'{base}results/predictions/annual_update/'\n",
    "year_start, year_end=2024,2024\n",
    "quantiles=[0.25,0.5,0.75] # interquartile range\n",
    "version = 'v2.0'\n",
    "target_grid='500m'\n",
    "dask_chunks=dict(x=500, y=500, time=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run step 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from _combine_ensemble import combine_ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for f,n in fluxes.items():\n",
    "    print(f)\n",
    "    # paths\n",
    "    predictions_folder= f'{predictions_path}{year_start}/{f}/'\n",
    "\n",
    "    # metadata for netcdf attributes\n",
    "    if f =='ET':\n",
    "        units = 'mm/month'\n",
    "    else:\n",
    "        units = 'gC/m\\N{SUPERSCRIPT TWO}/month'\n",
    "    \n",
    "    description = f'AusEFlux {n} is created by empirically upscaling the OzFlux eddy covariance network using machine learning methods coupled with climate and remote sensing datasets. The estimates provided within this dataset were extracted from an ensemble of predictions and represent the median and uncertainty range.'\n",
    "\n",
    "    # Create attributes dictionary\n",
    "    attrs_dict={}\n",
    "    attrs_dict['nodata'] = np.nan\n",
    "    attrs_dict['crs'] = 'EPSG:4326'\n",
    "    attrs_dict['short_name'] = f\n",
    "    attrs_dict['long_name'] = n\n",
    "    attrs_dict['units'] = units\n",
    "    attrs_dict['version'] = version\n",
    "    attrs_dict['description'] = description\n",
    "\n",
    "    #combine ensembles and save netcdf\n",
    "    combine_ensemble(\n",
    "        model_var=f,\n",
    "        results_path=f'{base}results/AusEFlux/{f}/',\n",
    "        predictions_folder=predictions_folder,\n",
    "        year_start=year_start,\n",
    "        year_end=year_end,\n",
    "        target_grid=target_grid,\n",
    "        dask_chunks=dask_chunks,\n",
    "        attrs=attrs_dict,\n",
    "        quantiles=quantiles,\n",
    "        verbose=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## You're done!\n",
    "\n",
    "**The next step** is to open and run `/g/data/xc0/project/AusEFlux/notebooks/Move_auseflux_to_production.ipynb` which will move the datasets into the production `ub8` folder, along with creating annual summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Last Run:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import date\n",
    "print(date.today())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
